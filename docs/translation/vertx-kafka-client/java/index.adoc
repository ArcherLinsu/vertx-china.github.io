= Vert.x Kafka 客户端
:toc: left

该组件提供了 Kafka 客户端， 可以用与给 link:https://kafka.apache.org/[Apache Kafka] 集群发送信息，或从中读取信息。

作为消费者，接口提供了订阅主题分区，并异步地
接收消息，或将消息作为流进行处理（甚至可以做到中止或重启数据流）的方法。

作为生产者，接口提供了流式向主题分区发送消息的方法。

== 使用 Vert.x 的 Kafka 客户端

为了使用该组件， 需要在您的构建描述文件中的依赖配置中添加如下内容：

* Maven (在您的 `pom.xml`):

[source,xml,subs="+attributes"]
----
<dependency>
 <groupId>io.vertx</groupId>
 <artifactId>vertx-kafka-client</artifactId>
 <version>4.0.3</version>
</dependency>
----

* Gradle (在您的 `build.gradle` 文件中):

[source,groovy,subs="+attributes"]
----
compile io.vertx:vertx-kafka-client:4.0.3
----

== 创建 kafka 客户端

创建 kafka 的消费者和生产者的方式非常详细，它们都基于原生的 kafka 的客户端库工作。

在创建时，需要进行很多配置，这些配置可以参考
Apache Kafka 文档， 参见 link:https://kafka.apache.org/documentation/#newconsumerconfigs[消费者] 和
 link:https://kafka.apache.org/documentation/#producerconfigs[生产者].

为了方便配置， 您可以将参数放置在一个 Map 容器中，并在调用
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` 和
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` 的静态创建方法时传入。

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
config.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");

// 使用消费者和 Apache Kafka 交互
KafkaConsumer<String, String> consumer = KafkaConsumer.create(vertx, config);
----

在以上代码中，我们传入了一个 Map 容器实例作为创建 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` 对象实例时
的参数，这样可以指定要连接的 kafka 节点列表（这里只有一个）的地址和
每个接收到的消息的键和内容的反序列化器。

创建 kafka 生产者地方法也大致相同。

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
config.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
config.put("acks", "1");

// 使用生产者和 Apache Kafka 交互
KafkaProducer<String, String> producer = KafkaProducer.create(vertx, config);
----

ifdef::java,groovy,kotlin[]
您也可以使用 `link:../../apidocs/java/util/Properties.html[Properties]` 来替代 Map 容器传递参数。

[source,java]
----
Properties config = new Properties();
config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
config.put(ConsumerConfig.GROUP_ID_CONFIG, "my_group");
config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
config.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");

// 使用消费者和 Apache Kafka 交互
KafkaConsumer<String, String> consumer = KafkaConsumer.create(vertx, config);
----

还有别的创建方法可以让您指定发送消息或接收消息的键和
数据的类型； 这样您可以直接设置键和数据的序列化/反序列化器
而不是使用属性来设置。

[source,java]
----
Properties config = new Properties();
config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
config.put(ProducerConfig.ACKS_CONFIG, "1");

// 使用生产者和 Apache Kafka 交互
KafkaProducer<String, String> producer = KafkaProducer.create(vertx, config, String.class, String.class);
----

以上是使用 `link:../../apidocs/java/util/Properties.html[Properties]` 来创建一个 `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` 实例的
代码，它指定了要连接的 Kafka 节点列表（目前只有一个）和消息确认的模式。消息的键和数据的反序列化器
通过 `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#create-io.vertx.core.Vertx-java.util.Properties-java.lang.Class-java.lang.Class-[KafkaProducer.create]` 方法的参数来设置。
endif::[]

== 加入一个消费者群组并从主题中接收消息

要开始从 kafka 的主题中接收消息， 消费者需要使用
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#subscribe-java.util.Set-[subscribe]` 方法去
作为一个消费者群组（群组在创建时的属性设置里指定）的一员去订阅一组主题。

您也可以使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#subscribe-java.util.regex.Pattern-[subscribe]` 方法去
指定一个正则表达式，并订阅所有匹配该正则表达式的主题。

为了注册一个处理器去处理接收到的消息，您需要使用
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#handler-io.vertx.core.Handler-[handler]` 方法。

[source,java]
----
consumer.handler(record -> {
  System.out.println("Processing key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());
});

// 订阅一组主题
Set<String> topics = new HashSet<>();
topics.add("topic1");
topics.add("topic2");
topics.add("topic3");
consumer.subscribe(topics);

// 或使用正则表达式
Pattern pattern = Pattern.compile("topic\\d");
consumer.subscribe(pattern);

// 或仅订阅一个主题
consumer.subscribe("a-single-topic");
----

您可以在调用 `subscribe()` 方法的前后注册消息处理器； 直到您调用了该方法并注册了消息处理器后，消息才会
开始被消费。 举个例子，您可以先调用 `subscribe()` 方法，再调用 `seek()` 方法，最后调用 `handler()` 方法
，这样您可以在一个特定的偏移处开始消费消息。

消息处理器也可以在订阅时注册，这样您就可以获取订阅的结果并当操作完成时
收到通知。

[source,java]
----
consumer.handler(record -> {
  System.out.println("Processing key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());
});

// 订阅一组主题
Set<String> topics = new HashSet<>();
topics.add("topic1");
topics.add("topic2");
topics.add("topic3");
consumer
  .subscribe(topics)
  .onSuccess(v ->
    System.out.println("subscribed")
  ).onFailure(cause ->
    System.out.println("Could not subscribe " + cause.getMessage())
  );

// 或仅订阅一个主题
consumer
  .subscribe("a-single-topic")
  .onSuccess(v ->
    System.out.println("subscribed")
  ).onFailure(cause ->
    System.out.println("Could not subscribe " + cause.getMessage())
  );
----

通过使用消费者群组，Kafka 集群会将同一个消费者群组下的其他消费者正在使用的分区
分配给该消费者， 因此分区可以在消费者群组中传播。

Kafka 集群会在消费者离开集群时（此时原消费者的分区可以分配给其他消费者）或
新的消费者加入集群时（新消费者的需要申请分区来读取）重新平衡分区。

您可以给 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` 注册一个处理器，这样
会在 kafka 集群给消费者分配或撤回主题分区时收到通知，使用
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsRevokedHandler-io.vertx.core.Handler-[partitionsRevokedHandler]` 和
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsAssignedHandler-io.vertx.core.Handler-[partitionsAssignedHandler]` 方法注册该处理器。

[source,java]
----
consumer.handler(record -> {
  System.out.println("Processing key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());
});

// 注册主题分区撤回和分配的处理器
consumer.partitionsAssignedHandler(topicPartitions -> {
  System.out.println("Partitions assigned");
  for (TopicPartition topicPartition : topicPartitions) {
    System.out.println(topicPartition.getTopic() + " " + topicPartition.getPartition());
  }
});

consumer.partitionsRevokedHandler(topicPartitions -> {
  System.out.println("Partitions revoked");
  for (TopicPartition topicPartition : topicPartitions) {
    System.out.println(topicPartition.getTopic() + " " + topicPartition.getPartition());
  }
});

// 订阅主题
consumer
  .subscribe("test")
  .onSuccess(v ->
    System.out.println("subscribed")
  ).onFailure(cause ->
    System.out.println("Could not subscribe " + cause.getMessage())
  );
----

在加入一个消费者群组接收消息后， 消费者可以选择使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#unsubscribe--[unsubscribe]` 方法
离开群组，这样就不会再收到消息

[source,java]
----
consumer.unsubscribe();
----

您还可以设置一个处理器来处理退出的结果

[source,java]
----
consumer
  .unsubscribe()
  .onSuccess(v ->
    System.out.println("Consumer unsubscribed")
  );
----

== 设置主题分区以接收消息

在接收消息时，除了加入消费者群组， 消费者也可以主动请求一个
主题分区。 当消费者并不在一个消费者群组内， 那么应用就不能
依赖 kafka 的重平衡特性。

您可以使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#assign-java.util.Set-io.vertx.core.Handler-[assign]` 方法
去请求特定的分区。

[source,java]
----
consumer.handler(record -> {
  System.out.println("key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());
});

//
Set<TopicPartition> topicPartitions = new HashSet<>();
topicPartitions.add(new TopicPartition()
  .setTopic("test")
  .setPartition(0));

// 请求分配特定的分区
consumer
  .assign(topicPartitions)
  .onSuccess(v -> System.out.println("Partition assigned"))
  // 成功后会从该分区获取消息
  .compose(v -> consumer.assignment())
  .onSuccess(partitions -> {
    for (TopicPartition topicPartition : partitions) {
      System.out.println(topicPartition.getTopic() + " " + topicPartition.getPartition());
    }
  });
----

使用 `subscribe()` 方法时， 您可以在调用 `assign()` 方法之前或之后注册接收消息处理器；
因为消息只会在两个方法都生效后才会被消费。 举个例子，您可以先调用
`assign()` 方法， 再调用 `seek()` 方法，最后调用 `handler()` 方法，
这样您就可以只消费特定分区的指定偏移之后的消息。

调用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#assignment-io.vertx.core.Handler-[assignment]` 可以让您
获取当前分配的消息分区。

== 通过显式请求接收消息

为了从 Kafka 接收消息，除了使用客户端内部自带的请求机制外， 客户端可以订阅
主题， 并且不注册消息处理器，并使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#poll-java.time.Duration-io.vertx.core.Handler-[poll]` 方法获取消息。

通过这种方式， 用户的应用可以在其需要时才执行请求以获取消息，
举个例子。

[source,java]
----
consumer
  .subscribe("test")
  .onSuccess(v -> {
    System.out.println("Consumer subscribed");

    // 每秒请求一次
    vertx.setPeriodic(1000, timerId ->
      consumer
        .poll(Duration.ofMillis(100))
        .onSuccess(records -> {
          for (int i = 0; i < records.size(); i++) {
            KafkaConsumerRecord<String, String> record = records.recordAt(i);
            System.out.println("key=" + record.key() + ",value=" + record.value() +
              ",partition=" + record.partition() + ",offset=" + record.offset());
          }
        })
        .onFailure(cause -> {
          System.out.println("Something went wrong when polling " + cause.toString());
          cause.printStackTrace();

          // 当发生错误时停止请求
          vertx.cancelTimer(timerId);
        })
    );
});
----

订阅成功后， 应用启动了一个定时器来执行请求并且
周期性地从 kafka 获取消息。

== Changing the subscription or assignment

You can change the subscribed topics, or assigned partitions after you have started to consume messages, simply
by calling `subscribe()` or `assign()` again.

Note that due to internal buffering of messages it is possible that the record handler will continue to
observe messages from the old subscription or assignment _after_ the `subscribe()` or `assign()`
method's completion handler has been called. This is not the case for messages observed by the batch handler:
Once the completion handler has been called it will only observe messages read from the subscription or assignment.

== Getting topic partition information

You can call the `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsFor-java.lang.String-io.vertx.core.Handler-[partitionsFor]` to get information about
partitions for a specified topic

[source,java]
----
consumer
  .partitionsFor("test")
  .onSuccess(partitions -> {
    for (PartitionInfo partitionInfo : partitions) {
      System.out.println(partitionInfo);
    }
  });
----

In addition `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#listTopics-io.vertx.core.Handler-[listTopics]` provides all available topics
with related partitions

[source,java]
----
consumer
  .listTopics()
  .onSuccess(partitionsTopicMap ->
    partitionsTopicMap.forEach((topic, partitions) -> {
      System.out.println("topic = " + topic);
      System.out.println("partitions = " + partitions);
    })
  );
----

== Manual offset commit

In Apache Kafka the consumer is in charge to handle the offset of the last read message.

This is executed by the commit operation executed automatically every time a bunch of messages are read
from a topic partition. The configuration parameter `enable.auto.commit` must be set to `true` when the
consumer is created.

Manual offset commit, can be achieved with `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#commit-io.vertx.core.Handler-[commit]`.
It can be used to achieve _at least once_ delivery to be sure that the read messages are processed before committing
the offset.

[source,java]
----
consumer.commit().onSuccess(v ->
  System.out.println("Last read message offset committed")
);
----

== Seeking in a topic partition

Apache Kafka can retain messages for a long period of time and the consumer can seek inside a topic partition
and obtain arbitrary access to the messages.

You can use `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seek-io.vertx.kafka.client.common.TopicPartition-long-[seek]` to change the offset for reading at a specific
position

[source,java]
----
TopicPartition topicPartition = new TopicPartition()
  .setTopic("test")
  .setPartition(0);

// seek to a specific offset
consumer
  .seek(topicPartition, 10)
  .onSuccess(v -> System.out.println("Seeking done"));
----

When the consumer needs to re-read the stream from the beginning, it can use `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToBeginning-io.vertx.kafka.client.common.TopicPartition-[seekToBeginning]`

[source,java]
----
TopicPartition topicPartition = new TopicPartition()
  .setTopic("test")
  .setPartition(0);

// seek to the beginning of the partition
consumer
  .seekToBeginning(Collections.singleton(topicPartition))
  .onSuccess(v -> System.out.println("Seeking done"));
----

Finally `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToEnd-io.vertx.kafka.client.common.TopicPartition-[seekToEnd]` can be used to come back at the end of the partition

[source,java]
----
TopicPartition topicPartition = new TopicPartition()
  .setTopic("test")
  .setPartition(0);

// seek to the end of the partition
consumer
  .seekToEnd(Collections.singleton(topicPartition))
  .onSuccess(v -> System.out.println("Seeking done"));
----

Note that due to internal buffering of messages it is possible that the record handler will continue to
observe messages read from the original offset for a time _after_ the `seek*()` method's completion
handler has been called. This is not the case for messages observed by the batch handler: Once the
`seek*()` completion handler has been called it will only observe messages read from the new offset.

== Offset lookup

You can use the beginningOffsets API introduced in Kafka 0.10.1.1 to get the first offset
for a given partition. In contrast to `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToBeginning-io.vertx.kafka.client.common.TopicPartition-[seekToBeginning]`,
it does not change the consumer's offset.

[source,java]
----
Set<TopicPartition> topicPartitions = new HashSet<>();
TopicPartition topicPartition = new TopicPartition().setTopic("test").setPartition(0);
topicPartitions.add(topicPartition);

consumer
  .beginningOffsets(topicPartitions)
  .onSuccess(results ->
    results.forEach((topic, beginningOffset) ->
      System.out.println(
        "Beginning offset for topic=" + topic.getTopic() + ", partition=" +
          topic.getPartition() + ", beginningOffset=" + beginningOffset
      )
    )
  );

// Convenience method for single-partition lookup
consumer
  .beginningOffsets(topicPartition)
  .onSuccess(beginningOffset ->
    System.out.println(
      "Beginning offset for topic=" + topicPartition.getTopic() + ", partition=" +
        topicPartition.getPartition() + ", beginningOffset=" + beginningOffset
    )
  );
----

You can use the endOffsets API introduced in Kafka 0.10.1.1 to get the last offset
for a given partition. In contrast to `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToEnd-io.vertx.kafka.client.common.TopicPartition-[seekToEnd]`,
it does not change the consumer's offset.

[source,java]
----
Set<TopicPartition> topicPartitions = new HashSet<>();
TopicPartition topicPartition = new TopicPartition().setTopic("test").setPartition(0);
topicPartitions.add(topicPartition);

consumer.endOffsets(topicPartitions)
  .onSuccess(results ->
    results.forEach((topic, beginningOffset) ->
      System.out.println(
        "End offset for topic=" + topic.getTopic() + ", partition=" +
          topic.getPartition() + ", beginningOffset=" + beginningOffset
      )
    )
  );

// Convenience method for single-partition lookup
consumer
  .endOffsets(topicPartition)
  .onSuccess(endOffset ->
    System.out.println(
      "End offset for topic=" + topicPartition.getTopic() + ", partition=" +
        topicPartition.getPartition() + ", endOffset=" + endOffset
    )
);
----

You can use the offsetsForTimes API introduced in Kafka 0.10.1.1 to look up an offset by
timestamp, i.e. search parameter is an epoch timestamp and the call returns the lowest offset
with ingestion timestamp >= given timestamp.

[source,java]
----
Map<TopicPartition, Long> topicPartitionsWithTimestamps = new HashMap<>();
TopicPartition topicPartition = new TopicPartition().setTopic("test").setPartition(0);

// We are interested in the offset for data ingested 60 seconds ago
long timestamp = (System.currentTimeMillis() - 60000);

topicPartitionsWithTimestamps.put(topicPartition, timestamp);
consumer
  .offsetsForTimes(topicPartitionsWithTimestamps)
  .onSuccess(results ->
    results.forEach((topic, offset) ->
      System.out.println(
        "Offset for topic=" + topic.getTopic() +
        ", partition=" + topic.getPartition() + "\n" +
        ", timestamp=" + timestamp + ", offset=" + offset.getOffset() +
        ", offsetTimestamp=" + offset.getTimestamp()
      )
    )
);

// Convenience method for single-partition lookup
consumer.offsetsForTimes(topicPartition, timestamp).onSuccess(offsetAndTimestamp ->
  System.out.println(
    "Offset for topic=" + topicPartition.getTopic() +
    ", partition=" + topicPartition.getPartition() + "\n" +
    ", timestamp=" + timestamp + ", offset=" + offsetAndTimestamp.getOffset() +
    ", offsetTimestamp=" + offsetAndTimestamp.getTimestamp()
  )
);
----
== Message flow control

A consumer can control the incoming message flow and pause/resume the read operation from a topic, e.g it
can pause the message flow when it needs more time to process the actual messages and then resume
to continue message processing.

To achieve that you can use `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#pause--[pause]` and
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#resume--[resume]`.

In the case of the partition-specific pause and resume it is possible that the record handler will continue to
observe messages from a paused partition for a time _after_ the `pause()` method's completion
handler has been called. This is not the case for messages observed by the batch handler: Once the
`pause()` completion handler has been called it will only observe messages from those partitions which
are not paused.

[source,java]
----
TopicPartition topicPartition = new TopicPartition()
  .setTopic("test")
  .setPartition(0);

// registering the handler for incoming messages
consumer.handler(record -> {
  System.out.println("key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());

  // i.e. pause/resume on partition 0, after reading message up to offset 5
  if ((record.partition() == 0) && (record.offset() == 5)) {

    // pause the read operations
    consumer.pause(topicPartition)
      .onSuccess(v -> System.out.println("Paused"))
      .onSuccess(v -> vertx.setTimer(5000, timeId ->
        // resume read operations
        consumer.resume(topicPartition)
      ));
  }
});
----

== Closing a consumer

Call close to close the consumer. Closing the consumer closes any open connections and releases all consumer resources.

The close is actually asynchronous and might not complete until some time after the call has returned. If you want to be notified
when the actual close has completed then you can pass in a handler.

This handler will then be called when the close has fully completed.

[source,java]
----
consumer
  .close()
  .onSuccess(v -> System.out.println("Consumer is now closed"))
  .onFailure(cause -> System.out.println("Close failed: " + cause));
----

== Sending messages to a topic

You can use  `link:../../apidocs/io/vertx/core/streams/WriteStream.html#write-java.lang.Object-[write]` to send messages (records) to a topic.

The simplest way to send a message is to specify only the destination topic and the related value, omitting its key
or partition, in this case the messages are sent in a round robin fashion across all the partitions of the topic.

[source,java]
----
for (int i = 0; i < 5; i++) {

  // only topic and message value are specified, round robin on destination partitions
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", "message_" + i);

  producer.write(record);
}
----

You can receive message sent metadata like its topic, its destination partition and its assigned offset.

[source,java]
----
for (int i = 0; i < 5; i++) {

  // only topic and message value are specified, round robin on destination partitions
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", "message_" + i);

  producer.send(record).onSuccess(recordMetadata ->
    System.out.println(
      "Message " + record.value() + " written on topic=" + recordMetadata.getTopic() +
      ", partition=" + recordMetadata.getPartition() +
      ", offset=" + recordMetadata.getOffset()
    )
  );
}
----

When you need to assign a partition to a message, you can specify its partition identifier
or its key

[source,java]
----
for (int i = 0; i < 10; i++) {

  // a destination partition is specified
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", null, "message_" + i, 0);

  producer.write(record);
}
----

Since the producers identifies the destination using key hashing, you can use that to guarantee that all
messages with the same key are sent to the same partition and retain the order.

[source,java]
----
for (int i = 0; i < 10; i++) {

  // i.e. defining different keys for odd and even messages
  int key = i % 2;

  // a key is specified, all messages with same key will be sent to the same partition
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", String.valueOf(key), "message_" + i);

  producer.write(record);
}
----

NOTE: the shared producer is created on the first `createShared` call and its configuration is defined at this moment,
shared producer usage must use the same configuration.

== Sharing a producer

Sometimes you want to share the same producer from within several verticles or contexts.

Calling `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#createShared-io.vertx.core.Vertx-java.lang.String-java.util.Map-[KafkaProducer.createShared]`
returns a producer that can be shared safely.

[source,java]
----
KafkaProducer<String, String> producer1 = KafkaProducer.createShared(vertx, "the-producer", config);

// Sometimes later you can close it
producer1.close();
----

The same resources (thread, connection) will be shared between the producer returned by this method.

When you are done with the producer, just close it, when all shared producers are closed, the resources will
be released for you.

== Closing a producer

Call close to close the producer. Closing the producer closes any open connections and releases all producer resources.

The close is actually asynchronous and might not complete until some time after the call has returned. If you want to be notified
when the actual close has completed then you can pass in a handler.

This handler will then be called when the close has fully completed.

[source,java]
----
producer
  .close()
  .onSuccess(v -> System.out.println("Producer is now closed"))
  .onFailure(cause -> System.out.println("Close failed: " + cause));
----

== Getting topic partition information

You can call the `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#partitionsFor-java.lang.String-io.vertx.core.Handler-[partitionsFor]` to get information about
partitions for a specified topic:

[source,java]
----
producer
  .partitionsFor("test")
  .onSuccess(partitions ->
    partitions.forEach(System.out::println)
  );
----

== Handling errors

Errors handling (e.g timeout) between a Kafka client (consumer or producer) and the Kafka cluster is done using
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#exceptionHandler-io.vertx.core.Handler-[exceptionHandler]` or
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#exceptionHandler-io.vertx.core.Handler-[exceptionHandler]`

[source,java]
----
consumer.exceptionHandler(e -> {
  System.out.println("Error = " + e.getMessage());
});
----

== Automatic clean-up in verticles

If you’re creating consumers and producer from inside verticles, those consumers and producers will be automatically
closed when the verticle is undeployed.

== Using Vert.x serializers/deserializers

Vert.x Kafka client comes out of the box with serializers and deserializers for buffers, json object
and json array.

In a consumer you can use buffers

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.deserializer", "io.vertx.kafka.client.serialization.BufferDeserializer");
config.put("value.deserializer", "io.vertx.kafka.client.serialization.BufferDeserializer");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");

// Creating a consumer able to deserialize to json object
config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.deserializer", "io.vertx.kafka.client.serialization.JsonObjectDeserializer");
config.put("value.deserializer", "io.vertx.kafka.client.serialization.JsonObjectDeserializer");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");

// Creating a consumer able to deserialize to json array
config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.deserializer", "io.vertx.kafka.client.serialization.JsonArrayDeserializer");
config.put("value.deserializer", "io.vertx.kafka.client.serialization.JsonArrayDeserializer");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");
----

Or in a producer

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.serializer", "io.vertx.kafka.client.serialization.BufferSerializer");
config.put("value.serializer", "io.vertx.kafka.client.serialization.BufferSerializer");
config.put("acks", "1");

// Creating a producer able to serialize to json object
config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.serializer", "io.vertx.kafka.client.serialization.JsonObjectSerializer");
config.put("value.serializer", "io.vertx.kafka.client.serialization.JsonObjectSerializer");
config.put("acks", "1");

// Creating a producer able to serialize to json array
config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.serializer", "io.vertx.kafka.client.serialization.JsonArraySerializer");
config.put("value.serializer", "io.vertx.kafka.client.serialization.JsonArraySerializer");
config.put("acks", "1");
----

ifeval::["java" == "java"]
You can also specify the serializers/deserializers at creation time:

In a consumer

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");

// Creating a consumer able to deserialize buffers
KafkaConsumer<Buffer, Buffer> bufferConsumer = KafkaConsumer.create(vertx, config, Buffer.class, Buffer.class);

// Creating a consumer able to deserialize json objects
KafkaConsumer<JsonObject, JsonObject> jsonObjectConsumer = KafkaConsumer.create(vertx, config, JsonObject.class, JsonObject.class);

// Creating a consumer able to deserialize json arrays
KafkaConsumer<JsonArray, JsonArray> jsonArrayConsumer = KafkaConsumer.create(vertx, config, JsonArray.class, JsonArray.class);
----

Or in a producer

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("acks", "1");

// Creating a producer able to serialize to buffers
KafkaProducer<Buffer, Buffer> bufferProducer = KafkaProducer.create(vertx, config, Buffer.class, Buffer.class);

// Creating a producer able to serialize to json objects
KafkaProducer<JsonObject, JsonObject> jsonObjectProducer = KafkaProducer.create(vertx, config, JsonObject.class, JsonObject.class);

// Creating a producer able to serialize to json arrays
KafkaProducer<JsonArray, JsonArray> jsonArrayProducer = KafkaProducer.create(vertx, config, JsonArray.class, JsonArray.class);
----

endif::[]

ifeval::["java" == "java"]
include::override/rxjava2.adoc[]
endif::[]

ifdef::java,groovy,kotlin[]
== Stream implementation and native Kafka objects

When you want to operate on native Kafka records you can use a stream oriented
implementation which handles native Kafka objects.

The `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaReadStream.html[KafkaReadStream]` shall be used for reading topic partitions, it is
a read stream of `link:../../apidocs/org/apache/kafka/clients/consumer/ConsumerRecord.html[ConsumerRecord]` objects.

The `link:../../apidocs/io/vertx/kafka/client/producer/KafkaWriteStream.html[KafkaWriteStream]` shall be used for writing to topics, it is a write
stream of `link:../../apidocs/org/apache/kafka/clients/producer/ProducerRecord.html[ProducerRecord]`.

The API exposed by these interfaces is mostly the same than the polyglot version.
endif::[]

== Automatic trace propagation

When Vert.x is configured with Tracing enabled (see `link:../../apidocs/io/vertx/core/VertxOptions.html#setTracingOptions-io.vertx.core.tracing.TracingOptions-[setTracingOptions]`),
traces will be automatically propagated with Kafka messages.

Kafka producers will add a span to traces when a message is being written, trace contexts
will be propagated as Kafka headers, and the consumers will also create a span while reading a message.

Following the
link:https://github.com/opentracing/specification/blob/master/semantic_conventions.md[OpenTracing semantic convention],
span tags are:

- `span.kind`, it is either `consumer` or `producer`
- `peer.address` can be configured from `link:../../apidocs/io/vertx/kafka/client/common/KafkaClientOptions.html#setTracePeerAddress-java.lang.String-[setTracePeerAddress]`. If unset, it will be the configured bootstrap server.
- `peer.hostname` is parsed from `peer.address`
- `peer.port` is parsed from `peer.address`
- `peer.service` is always `kafka`
- `message_bus.destination`, which is set to the topic in use

include::admin.adoc[]