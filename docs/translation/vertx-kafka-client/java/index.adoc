= Vert.x Kafka 客户端
:toc: left

该组件提供了 Kafka 客户端， 可以用与给 link:https://kafka.apache.org/[Apache Kafka] 集群发送信息，或从中读取信息。

作为消费者，接口提供了订阅主题分区，并异步地
接收消息，或将消息作为流进行处理（甚至可以做到中止或重启数据流）的方法。

作为生产者，接口提供了流式向主题分区发送消息的方法。

== 使用 Vert.x 的 Kafka 客户端

为了使用该组件， 需要在您的构建描述文件中的依赖配置中添加如下内容：

* Maven (在您的 `pom.xml`):

[source,xml,subs="+attributes"]
----
<dependency>
 <groupId>io.vertx</groupId>
 <artifactId>vertx-kafka-client</artifactId>
 <version>4.0.3</version>
</dependency>
----

* Gradle (在您的 `build.gradle` 文件中):

[source,groovy,subs="+attributes"]
----
compile io.vertx:vertx-kafka-client:4.0.3
----

== 创建 kafka 客户端

创建 kafka 的消费者和生产者的方式非常详细，它们都基于原生的 kafka 的客户端库工作。

在创建时，需要进行很多配置，这些配置可以参考
Apache Kafka 文档， 参见 link:https://kafka.apache.org/documentation/#newconsumerconfigs[消费者] 和
 link:https://kafka.apache.org/documentation/#producerconfigs[生产者].

为了方便配置， 您可以将参数放置在一个 Map 容器中，并在调用
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` 和
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` 的静态创建方法时传入。

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
config.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");

// 使用消费者和 Apache Kafka 交互
KafkaConsumer<String, String> consumer = KafkaConsumer.create(vertx, config);
----

在以上代码中，我们传入了一个 Map 容器实例作为创建 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` 对象实例时
的参数，这样可以指定要连接的 kafka 节点列表（这里只有一个）的地址和
每个接收到的消息的键和内容的反序列化器。

创建 kafka 生产者地方法也大致相同。

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
config.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
config.put("acks", "1");

// 使用生产者和 Apache Kafka 交互
KafkaProducer<String, String> producer = KafkaProducer.create(vertx, config);
----

ifdef::java,groovy,kotlin[]
您也可以使用 `link:../../apidocs/java/util/Properties.html[Properties]` 来替代 Map 容器传递参数。

[source,java]
----
Properties config = new Properties();
config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
config.put(ConsumerConfig.GROUP_ID_CONFIG, "my_group");
config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
config.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");

// 使用消费者和 Apache Kafka 交互
KafkaConsumer<String, String> consumer = KafkaConsumer.create(vertx, config);
----

还有别的创建方法可以让您指定发送消息或接收消息的键和
数据的类型； 这样您可以直接设置键和数据的序列化/反序列化器
而不是使用属性来设置。

[source,java]
----
Properties config = new Properties();
config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
config.put(ProducerConfig.ACKS_CONFIG, "1");

// 使用生产者和 Apache Kafka 交互
KafkaProducer<String, String> producer = KafkaProducer.create(vertx, config, String.class, String.class);
----

以上是使用 `link:../../apidocs/java/util/Properties.html[Properties]` 来创建一个 `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html[KafkaProducer]` 实例的
代码，它指定了要连接的 Kafka 节点列表（目前只有一个）和消息确认的模式。消息的键和数据的反序列化器
通过 `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#create-io.vertx.core.Vertx-java.util.Properties-java.lang.Class-java.lang.Class-[KafkaProducer.create]` 方法的参数来设置。
endif::[]

== 加入一个消费者群组并从主题中接收消息

要开始从 kafka 的主题中接收消息， 消费者需要使用
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#subscribe-java.util.Set-[subscribe]` 方法去
作为一个消费者群组（群组在创建时的属性设置里指定）的一员去订阅一组主题。

您也可以使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#subscribe-java.util.regex.Pattern-[subscribe]` 方法去
指定一个正则表达式，并订阅所有匹配该正则表达式的主题。

为了注册一个处理器去处理接收到的消息，您需要使用
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#handler-io.vertx.core.Handler-[handler]` 方法。

[source,java]
----
consumer.handler(record -> {
  System.out.println("Processing key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());
});

// 订阅一组主题
Set<String> topics = new HashSet<>();
topics.add("topic1");
topics.add("topic2");
topics.add("topic3");
consumer.subscribe(topics);

// 或使用正则表达式
Pattern pattern = Pattern.compile("topic\\d");
consumer.subscribe(pattern);

// 或仅订阅一个主题
consumer.subscribe("a-single-topic");
----

您可以在调用 `subscribe()` 方法的前后注册消息处理器； 直到您调用了该方法并注册了消息处理器后，消息才会
开始被消费。 举个例子，您可以先调用 `subscribe()` 方法，再调用 `seek()` 方法，最后调用 `handler()` 方法
，这样您可以在一个特定的偏移处开始消费消息。

消息处理器也可以在订阅时注册，这样您就可以获取订阅的结果并当操作完成时
收到通知。

[source,java]
----
consumer.handler(record -> {
  System.out.println("Processing key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());
});

// 订阅一组主题
Set<String> topics = new HashSet<>();
topics.add("topic1");
topics.add("topic2");
topics.add("topic3");
consumer
  .subscribe(topics)
  .onSuccess(v ->
    System.out.println("subscribed")
  ).onFailure(cause ->
    System.out.println("Could not subscribe " + cause.getMessage())
  );

// 或仅订阅一个主题
consumer
  .subscribe("a-single-topic")
  .onSuccess(v ->
    System.out.println("subscribed")
  ).onFailure(cause ->
    System.out.println("Could not subscribe " + cause.getMessage())
  );
----

通过使用消费者群组，Kafka 集群会将同一个消费者群组下的其他消费者正在使用的分区
分配给该消费者， 因此分区可以在消费者群组中传播。

Kafka 集群会在消费者离开集群时（此时原消费者的分区可以分配给其他消费者）或
新的消费者加入集群时（新消费者的需要申请分区来读取）重新平衡分区。

您可以给 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html[KafkaConsumer]` 注册一个处理器，这样
会在 kafka 集群给消费者分配或撤回主题分区时收到通知，使用
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsRevokedHandler-io.vertx.core.Handler-[partitionsRevokedHandler]` 和
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsAssignedHandler-io.vertx.core.Handler-[partitionsAssignedHandler]` 方法注册该处理器。

[source,java]
----
consumer.handler(record -> {
  System.out.println("Processing key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());
});

// 注册主题分区撤回和分配的处理器
consumer.partitionsAssignedHandler(topicPartitions -> {
  System.out.println("Partitions assigned");
  for (TopicPartition topicPartition : topicPartitions) {
    System.out.println(topicPartition.getTopic() + " " + topicPartition.getPartition());
  }
});

consumer.partitionsRevokedHandler(topicPartitions -> {
  System.out.println("Partitions revoked");
  for (TopicPartition topicPartition : topicPartitions) {
    System.out.println(topicPartition.getTopic() + " " + topicPartition.getPartition());
  }
});

// 订阅主题
consumer
  .subscribe("test")
  .onSuccess(v ->
    System.out.println("subscribed")
  ).onFailure(cause ->
    System.out.println("Could not subscribe " + cause.getMessage())
  );
----

在加入一个消费者群组接收消息后， 消费者可以选择使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#unsubscribe--[unsubscribe]` 方法
离开群组，这样就不会再收到消息

[source,java]
----
consumer.unsubscribe();
----

您还可以设置一个处理器来处理退出的结果

[source,java]
----
consumer
  .unsubscribe()
  .onSuccess(v ->
    System.out.println("Consumer unsubscribed")
  );
----

== 设置主题分区以接收消息

在接收消息时，除了加入消费者群组， 消费者也可以主动请求一个
主题分区。 当消费者并不在一个消费者群组内， 那么应用就不能
依赖 kafka 的重平衡特性。

您可以使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#assign-java.util.Set-io.vertx.core.Handler-[assign]` 方法
去请求特定的分区。

[source,java]
----
consumer.handler(record -> {
  System.out.println("key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());
});

//
Set<TopicPartition> topicPartitions = new HashSet<>();
topicPartitions.add(new TopicPartition()
  .setTopic("test")
  .setPartition(0));

// 请求分配特定的分区
consumer
  .assign(topicPartitions)
  .onSuccess(v -> System.out.println("Partition assigned"))
  // 成功后会从该分区获取消息
  .compose(v -> consumer.assignment())
  .onSuccess(partitions -> {
    for (TopicPartition topicPartition : partitions) {
      System.out.println(topicPartition.getTopic() + " " + topicPartition.getPartition());
    }
  });
----

使用 `subscribe()` 方法时， 您可以在调用 `assign()` 方法之前或之后注册接收消息处理器；
因为消息只会在两个方法都生效后才会被消费。 举个例子，您可以先调用
`assign()` 方法， 再调用 `seek()` 方法，最后调用 `handler()` 方法，
这样您就可以只消费特定分区的指定偏移之后的消息。

调用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#assignment-io.vertx.core.Handler-[assignment]` 可以让您
获取当前分配的消息分区。

== 通过显式请求接收消息

为了从 Kafka 接收消息，除了使用客户端内部自带的请求机制外， 客户端可以订阅
主题， 并且不注册消息处理器，并使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#poll-java.time.Duration-io.vertx.core.Handler-[poll]` 方法获取消息。

通过这种方式， 用户的应用可以在其需要时才执行请求以获取消息，
举个例子。

[source,java]
----
consumer
  .subscribe("test")
  .onSuccess(v -> {
    System.out.println("Consumer subscribed");

    // 每秒请求一次
    vertx.setPeriodic(1000, timerId ->
      consumer
        .poll(Duration.ofMillis(100))
        .onSuccess(records -> {
          for (int i = 0; i < records.size(); i++) {
            KafkaConsumerRecord<String, String> record = records.recordAt(i);
            System.out.println("key=" + record.key() + ",value=" + record.value() +
              ",partition=" + record.partition() + ",offset=" + record.offset());
          }
        })
        .onFailure(cause -> {
          System.out.println("Something went wrong when polling " + cause.toString());
          cause.printStackTrace();

          // 当发生错误时停止请求
          vertx.cancelTimer(timerId);
        })
    );
});
----

订阅成功后， 应用启动了一个定时器来执行请求并且
周期性地从 kafka 获取消息。

== 改变订阅或主题分区的分配

您可以在开始消费消息之后修改订阅的主题或主题分区的分配，只需要
重新调用 `subscribe()` 方法或 `assign()` 方法。

请记住，由于 kafka 客户端的内部存在消息缓存， 因此很有可能在您
调用 `subscribe()` 方法或 `assign()` 方法 _之后_ ，原先的消息处理器仍然 
收到了旧的主题或分区的消息。 但是如果您使用了批处理器就不会发生这种情况：
一旦重新调用订阅或修改方法的完成回调被触发， 那么客户端就只会收到新的主题或分区的消息。

== 获取主题分区信息

您可以调用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#partitionsFor-java.lang.String-io.vertx.core.Handler-[partitionsFor]` 方法来获取
特定主题的分区信息。

[source,java]
----
consumer
  .partitionsFor("test")
  .onSuccess(partitions -> {
    for (PartitionInfo partitionInfo : partitions) {
      System.out.println(partitionInfo);
    }
  });
----

您也可以调用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#listTopics-io.vertx.core.Handler-[listTopics]` 方法获取所有当前主题的
分区信息。

[source,java]
----
consumer
  .listTopics()
  .onSuccess(partitionsTopicMap ->
    partitionsTopicMap.forEach((topic, partitions) -> {
      System.out.println("topic = " + topic);
      System.out.println("partitions = " + partitions);
    })
  );
----

== 手动提交偏移

Apache Kafka 的消费者一般会处理最后一个读取的消息的偏移。

一般情况下，kafka 的客户端会自动地在每次从主题分区获取一批消息
后通过提交操作处理。 配置参数 `enable.auto.commit` 会在客户端被创建时设置
为 `true` 。

手动提交偏移，可以使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#commit-io.vertx.core.Handler-[commit]` 方法。
这样可以确保 _至少一次_ 提交偏移前消息已经被
处理了。

[source,java]
----
consumer.commit().onSuccess(v ->
  System.out.println("Last read message offset committed")
);
----

== 在消息分区内查询

Apache Kafka 可以保存一段时间内的消息数据，并且消费者可以在消息分区内查询
并获取任意一条消息。

您可以使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seek-io.vertx.kafka.client.common.TopicPartition-long-[seek]` 方法来改变读取时的偏移，并移动到
特定的位置

[source,java]
----
TopicPartition topicPartition = new TopicPartition()
  .setTopic("test")
  .setPartition(0);

// 移动特定的偏移
consumer
  .seek(topicPartition, 10)
  .onSuccess(v -> System.out.println("Seeking done"));
----

当消费者需要从开始处重新获取消息时，可以使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToBeginning-io.vertx.kafka.client.common.TopicPartition-[seekToBeginning]`

[source,java]
----
TopicPartition topicPartition = new TopicPartition()
  .setTopic("test")
  .setPartition(0);

// 移动偏移到分区开头
consumer
  .seekToBeginning(Collections.singleton(topicPartition))
  .onSuccess(v -> System.out.println("Seeking done"));
----

最后，`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToEnd-io.vertx.kafka.client.common.TopicPartition-[seekToEnd]` 可以用于将偏移移动到分区的结尾

[source,java]
----
TopicPartition topicPartition = new TopicPartition()
  .setTopic("test")
  .setPartition(0);

// 移动偏移到分区末尾
consumer
  .seekToEnd(Collections.singleton(topicPartition))
  .onSuccess(v -> System.out.println("Seeking done"));
----

请记住，由于 kafka 客户端的内部存在消息缓存， 因此很有可能在您
调用完 `seek*()` 方法 _之后_ 原有的消息处理器仍在获取原先
偏移处的消息。 但是如果您使用了批处理器就不会发生这种情况： 一旦
`seek*()` 的完成回调被触发， 消息处理器就只会接收到新的偏移处的消息。

== 查询偏移

您可以使用在 Kafka 0.10.1.1 引入的 beginningOffsets 接口来获取指定分区的
第一个偏移。 与 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToBeginning-io.vertx.kafka.client.common.TopicPartition-[seekToBeginning]` 方法不同的是，
该接口并不会改变当前客户端的偏移。

[source,java]
----
Set<TopicPartition> topicPartitions = new HashSet<>();
TopicPartition topicPartition = new TopicPartition().setTopic("test").setPartition(0);
topicPartitions.add(topicPartition);

consumer
  .beginningOffsets(topicPartitions)
  .onSuccess(results ->
    results.forEach((topic, beginningOffset) ->
      System.out.println(
        "Beginning offset for topic=" + topic.getTopic() + ", partition=" +
          topic.getPartition() + ", beginningOffset=" + beginningOffset
      )
    )
  );

// 方便地获取一个分区的偏移
consumer
  .beginningOffsets(topicPartition)
  .onSuccess(beginningOffset ->
    System.out.println(
      "Beginning offset for topic=" + topicPartition.getTopic() + ", partition=" +
        topicPartition.getPartition() + ", beginningOffset=" + beginningOffset
    )
  );
----

您可以使用在 Kafka 0.10.1.1 引入的 endOffsets 接口来获取指定分区的
结尾偏移。 与 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#seekToEnd-io.vertx.kafka.client.common.TopicPartition-[seekToEnd]` 方法不同的是，
该接口并不会改变当前客户端的偏移。

[source,java]
----
Set<TopicPartition> topicPartitions = new HashSet<>();
TopicPartition topicPartition = new TopicPartition().setTopic("test").setPartition(0);
topicPartitions.add(topicPartition);

consumer.endOffsets(topicPartitions)
  .onSuccess(results ->
    results.forEach((topic, beginningOffset) ->
      System.out.println(
        "End offset for topic=" + topic.getTopic() + ", partition=" +
          topic.getPartition() + ", beginningOffset=" + beginningOffset
      )
    )
  );

// 方便地获取一个分区的偏移
consumer
  .endOffsets(topicPartition)
  .onSuccess(endOffset ->
    System.out.println(
      "End offset for topic=" + topicPartition.getTopic() + ", partition=" +
        topicPartition.getPartition() + ", endOffset=" + endOffset
    )
);
----

您可以使用在 Kafka 0.10.1.1 引入的 endOffsets 接口来根据时间戳获取指定分区的
偏移。查询参数是一个 unix 时间戳，而返回的结果是满足
摄入时间 >= 给定时间条件的最小偏移。

[source,java]
----
Map<TopicPartition, Long> topicPartitionsWithTimestamps = new HashMap<>();
TopicPartition topicPartition = new TopicPartition().setTopic("test").setPartition(0);

// 我们想知道 60 秒前摄入消息的偏移
long timestamp = (System.currentTimeMillis() - 60000);

topicPartitionsWithTimestamps.put(topicPartition, timestamp);
consumer
  .offsetsForTimes(topicPartitionsWithTimestamps)
  .onSuccess(results ->
    results.forEach((topic, offset) ->
      System.out.println(
        "Offset for topic=" + topic.getTopic() +
        ", partition=" + topic.getPartition() + "\n" +
        ", timestamp=" + timestamp + ", offset=" + offset.getOffset() +
        ", offsetTimestamp=" + offset.getTimestamp()
      )
    )
);

// 方便地获取一个分区的偏移
consumer.offsetsForTimes(topicPartition, timestamp).onSuccess(offsetAndTimestamp ->
  System.out.println(
    "Offset for topic=" + topicPartition.getTopic() +
    ", partition=" + topicPartition.getPartition() + "\n" +
    ", timestamp=" + timestamp + ", offset=" + offsetAndTimestamp.getOffset() +
    ", offsetTimestamp=" + offsetAndTimestamp.getTimestamp()
  )
);
----
== 消息流控制

kafka 的消费者可以控制消息的流入，并且暂停 / 重启从一个主题中读取消息的操作。当消费者需要
更多时间去处理当前消息时，它可以暂停消息流，它也可以重启消息流
去继续处理消息。

为了这么做，你可以使用 `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#pause--[pause]` 方法和
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#resume--[resume]` 方法。

在对特定的主题分区调用了暂停和重启方法后，消息处理器有可能仍然会从
已经暂停了的主题分区接收消息，即使是在 `pause()` 方法的完成回调
_已经被调用之后_ 。 如果你使用了批处理器，一旦你调用
`pause()` 方法的完成回调被调用， 消费者就只能从未被暂停的主题分区
接收消息。

[source,java]
----
TopicPartition topicPartition = new TopicPartition()
  .setTopic("test")
  .setPartition(0);

// 注册消息处理器
consumer.handler(record -> {
  System.out.println("key=" + record.key() + ",value=" + record.value() +
    ",partition=" + record.partition() + ",offset=" + record.offset());

  // 在接收消息的偏移到达 5 之后暂停 / 重启分区 0 的消息流
  if ((record.partition() == 0) && (record.offset() == 5)) {

    // pause the read operations
    consumer.pause(topicPartition)
      .onSuccess(v -> System.out.println("Paused"))
      .onSuccess(v -> vertx.setTimer(5000, timeId ->
        // 重启读操作
        consumer.resume(topicPartition)
      ));
  }
});
----

== 关闭消费者

调用 close 方法来关闭消费者。 关闭消费者会关闭其所持有的所有连接并释放它所有的消费者资源。

close 方法是异步的并且在方法返回时可能还未完成。 如果您想在关闭完成后
收到通知，那么可以向其传递一个回调处理器。

该回调处理器会在关闭操作完全完成后被调用。

[source,java]
----
consumer
  .close()
  .onSuccess(v -> System.out.println("Consumer is now closed"))
  .onFailure(cause -> System.out.println("Close failed: " + cause));
----

== Sending messages to a topic

You can use  `link:../../apidocs/io/vertx/core/streams/WriteStream.html#write-java.lang.Object-[write]` to send messages (records) to a topic.

The simplest way to send a message is to specify only the destination topic and the related value, omitting its key
or partition, in this case the messages are sent in a round robin fashion across all the partitions of the topic.

[source,java]
----
for (int i = 0; i < 5; i++) {

  // only topic and message value are specified, round robin on destination partitions
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", "message_" + i);

  producer.write(record);
}
----

You can receive message sent metadata like its topic, its destination partition and its assigned offset.

[source,java]
----
for (int i = 0; i < 5; i++) {

  // only topic and message value are specified, round robin on destination partitions
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", "message_" + i);

  producer.send(record).onSuccess(recordMetadata ->
    System.out.println(
      "Message " + record.value() + " written on topic=" + recordMetadata.getTopic() +
      ", partition=" + recordMetadata.getPartition() +
      ", offset=" + recordMetadata.getOffset()
    )
  );
}
----

When you need to assign a partition to a message, you can specify its partition identifier
or its key

[source,java]
----
for (int i = 0; i < 10; i++) {

  // a destination partition is specified
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", null, "message_" + i, 0);

  producer.write(record);
}
----

Since the producers identifies the destination using key hashing, you can use that to guarantee that all
messages with the same key are sent to the same partition and retain the order.

[source,java]
----
for (int i = 0; i < 10; i++) {

  // i.e. defining different keys for odd and even messages
  int key = i % 2;

  // a key is specified, all messages with same key will be sent to the same partition
  KafkaProducerRecord<String, String> record =
    KafkaProducerRecord.create("test", String.valueOf(key), "message_" + i);

  producer.write(record);
}
----

NOTE: the shared producer is created on the first `createShared` call and its configuration is defined at this moment,
shared producer usage must use the same configuration.

== Sharing a producer

Sometimes you want to share the same producer from within several verticles or contexts.

Calling `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#createShared-io.vertx.core.Vertx-java.lang.String-java.util.Map-[KafkaProducer.createShared]`
returns a producer that can be shared safely.

[source,java]
----
KafkaProducer<String, String> producer1 = KafkaProducer.createShared(vertx, "the-producer", config);

// Sometimes later you can close it
producer1.close();
----

The same resources (thread, connection) will be shared between the producer returned by this method.

When you are done with the producer, just close it, when all shared producers are closed, the resources will
be released for you.

== Closing a producer

Call close to close the producer. Closing the producer closes any open connections and releases all producer resources.

The close is actually asynchronous and might not complete until some time after the call has returned. If you want to be notified
when the actual close has completed then you can pass in a handler.

This handler will then be called when the close has fully completed.

[source,java]
----
producer
  .close()
  .onSuccess(v -> System.out.println("Producer is now closed"))
  .onFailure(cause -> System.out.println("Close failed: " + cause));
----

== Getting topic partition information

You can call the `link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#partitionsFor-java.lang.String-io.vertx.core.Handler-[partitionsFor]` to get information about
partitions for a specified topic:

[source,java]
----
producer
  .partitionsFor("test")
  .onSuccess(partitions ->
    partitions.forEach(System.out::println)
  );
----

== Handling errors

Errors handling (e.g timeout) between a Kafka client (consumer or producer) and the Kafka cluster is done using
`link:../../apidocs/io/vertx/kafka/client/consumer/KafkaConsumer.html#exceptionHandler-io.vertx.core.Handler-[exceptionHandler]` or
`link:../../apidocs/io/vertx/kafka/client/producer/KafkaProducer.html#exceptionHandler-io.vertx.core.Handler-[exceptionHandler]`

[source,java]
----
consumer.exceptionHandler(e -> {
  System.out.println("Error = " + e.getMessage());
});
----

== Automatic clean-up in verticles

If you’re creating consumers and producer from inside verticles, those consumers and producers will be automatically
closed when the verticle is undeployed.

== Using Vert.x serializers/deserializers

Vert.x Kafka client comes out of the box with serializers and deserializers for buffers, json object
and json array.

In a consumer you can use buffers

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.deserializer", "io.vertx.kafka.client.serialization.BufferDeserializer");
config.put("value.deserializer", "io.vertx.kafka.client.serialization.BufferDeserializer");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");

// Creating a consumer able to deserialize to json object
config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.deserializer", "io.vertx.kafka.client.serialization.JsonObjectDeserializer");
config.put("value.deserializer", "io.vertx.kafka.client.serialization.JsonObjectDeserializer");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");

// Creating a consumer able to deserialize to json array
config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.deserializer", "io.vertx.kafka.client.serialization.JsonArrayDeserializer");
config.put("value.deserializer", "io.vertx.kafka.client.serialization.JsonArrayDeserializer");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");
----

Or in a producer

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.serializer", "io.vertx.kafka.client.serialization.BufferSerializer");
config.put("value.serializer", "io.vertx.kafka.client.serialization.BufferSerializer");
config.put("acks", "1");

// Creating a producer able to serialize to json object
config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.serializer", "io.vertx.kafka.client.serialization.JsonObjectSerializer");
config.put("value.serializer", "io.vertx.kafka.client.serialization.JsonObjectSerializer");
config.put("acks", "1");

// Creating a producer able to serialize to json array
config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("key.serializer", "io.vertx.kafka.client.serialization.JsonArraySerializer");
config.put("value.serializer", "io.vertx.kafka.client.serialization.JsonArraySerializer");
config.put("acks", "1");
----

ifeval::["java" == "java"]
You can also specify the serializers/deserializers at creation time:

In a consumer

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("group.id", "my_group");
config.put("auto.offset.reset", "earliest");
config.put("enable.auto.commit", "false");

// Creating a consumer able to deserialize buffers
KafkaConsumer<Buffer, Buffer> bufferConsumer = KafkaConsumer.create(vertx, config, Buffer.class, Buffer.class);

// Creating a consumer able to deserialize json objects
KafkaConsumer<JsonObject, JsonObject> jsonObjectConsumer = KafkaConsumer.create(vertx, config, JsonObject.class, JsonObject.class);

// Creating a consumer able to deserialize json arrays
KafkaConsumer<JsonArray, JsonArray> jsonArrayConsumer = KafkaConsumer.create(vertx, config, JsonArray.class, JsonArray.class);
----

Or in a producer

[source,java]
----
Map<String, String> config = new HashMap<>();
config.put("bootstrap.servers", "localhost:9092");
config.put("acks", "1");

// Creating a producer able to serialize to buffers
KafkaProducer<Buffer, Buffer> bufferProducer = KafkaProducer.create(vertx, config, Buffer.class, Buffer.class);

// Creating a producer able to serialize to json objects
KafkaProducer<JsonObject, JsonObject> jsonObjectProducer = KafkaProducer.create(vertx, config, JsonObject.class, JsonObject.class);

// Creating a producer able to serialize to json arrays
KafkaProducer<JsonArray, JsonArray> jsonArrayProducer = KafkaProducer.create(vertx, config, JsonArray.class, JsonArray.class);
----

endif::[]

ifeval::["java" == "java"]
include::override/rxjava2.adoc[]
endif::[]

ifdef::java,groovy,kotlin[]
== Stream implementation and native Kafka objects

When you want to operate on native Kafka records you can use a stream oriented
implementation which handles native Kafka objects.

The `link:../../apidocs/io/vertx/kafka/client/consumer/KafkaReadStream.html[KafkaReadStream]` shall be used for reading topic partitions, it is
a read stream of `link:../../apidocs/org/apache/kafka/clients/consumer/ConsumerRecord.html[ConsumerRecord]` objects.

The `link:../../apidocs/io/vertx/kafka/client/producer/KafkaWriteStream.html[KafkaWriteStream]` shall be used for writing to topics, it is a write
stream of `link:../../apidocs/org/apache/kafka/clients/producer/ProducerRecord.html[ProducerRecord]`.

The API exposed by these interfaces is mostly the same than the polyglot version.
endif::[]

== Automatic trace propagation

When Vert.x is configured with Tracing enabled (see `link:../../apidocs/io/vertx/core/VertxOptions.html#setTracingOptions-io.vertx.core.tracing.TracingOptions-[setTracingOptions]`),
traces will be automatically propagated with Kafka messages.

Kafka producers will add a span to traces when a message is being written, trace contexts
will be propagated as Kafka headers, and the consumers will also create a span while reading a message.

Following the
link:https://github.com/opentracing/specification/blob/master/semantic_conventions.md[OpenTracing semantic convention],
span tags are:

- `span.kind`, it is either `consumer` or `producer`
- `peer.address` can be configured from `link:../../apidocs/io/vertx/kafka/client/common/KafkaClientOptions.html#setTracePeerAddress-java.lang.String-[setTracePeerAddress]`. If unset, it will be the configured bootstrap server.
- `peer.hostname` is parsed from `peer.address`
- `peer.port` is parsed from `peer.address`
- `peer.service` is always `kafka`
- `message_bus.destination`, which is set to the topic in use

include::admin.adoc[]